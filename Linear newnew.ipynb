{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c681f62d",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression Tutorial with Ames Housing Dataset  \n",
    "\n",
    "This notebook will guide you step by step through **linear regression**, from simple to more complex models, using the Ames Housing dataset.  \n",
    "\n",
    "Learning Objectives:  \n",
    "- Understand the basics of **linear regression** for predictive modeling.  \n",
    "- Implement **simple linear regression** with one feature and interpret coefficients.  \n",
    "- Extend to **multiple linear regression** with several features and evaluate performance.  \n",
    "- Explore **polynomial regression** to capture nonlinear relationships.  \n",
    "- Recognize the problem of **overfitting** when using higher-degree polynomials.  \n",
    "- Apply **Ridge regularization** to control overfitting and balance the bias‚Äìvariance tradeoff.  \n",
    "- Evaluate models using **RMSE**, **R¬≤**, and **Predicted vs Observed plots**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83cda8",
   "metadata": {},
   "source": [
    "## 1.1 Load Dataset  \n",
    "\n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **fetch_openml()**  \n",
    "  - Function from scikit-learn to download datasets from the [OpenML repository](https://www.openml.org/).  \n",
    "  - Setting `as_frame=True` returns the dataset as a **pandas DataFrame**, making it easier to explore with pandas tools.  \n",
    "  \n",
    "- **Ames Housing Dataset**  \n",
    "  - Contains **1460 houses √ó 81 columns**.  \n",
    "  - **Features (80 columns):** lot size, number of rooms, year built, etc.  \n",
    "  - **Target (1 column):** `SalePrice`, the house price we want to predict.  \n",
    "  - Often used as a benchmark in regression tutorials. \n",
    "\n",
    "- **Suppress warnings**  \n",
    "  - `warnings.filterwarnings()` hides solver/runtime warnings that may appear (especially with high-degree polynomials), keeping the notebook output clean.  \n",
    "\n",
    "- **First look at the data**  \n",
    "  - `print(\"Shape:\", df.shape)` shows dataset dimensions.  \n",
    "  - `df.head()` displays the first 5 rows for inspection.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a69dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa7451",
   "metadata": {},
   "source": [
    "## 1.2 Select and Visualize Features  \n",
    "\n",
    "üí° **Tips:**   \n",
    "- **Feature selection**: \n",
    "- Use `pandas` to select relevant numeric columns. \n",
    "- We pick a few numeric columns that are likely related to house price (check the [explanation](https://www.kaggle.com/competitions/regression-techniques-house-prices/data) of each feature):\n",
    "  - `GrLivArea` (living area in sqft)  \n",
    "  - `BedroomAbvGr` (bedrooms above ground)  \n",
    "  - `GarageCars` (garage capacity)  \n",
    "  - `FullBath` (full bathrooms)  \n",
    "  - `LotArea` (lot size in sqft)  \n",
    "  - `YearBuilt` (year the house was built)  \n",
    "  - `TotRmsAbvGrd` (total rooms above ground)  \n",
    "  - `Fireplaces` (number of fireplaces)  \n",
    "- **Target variable**:  \n",
    "  - We use `SalePrice`, scaled down by 1000 for readability (so prices are in thousands).  \n",
    "- **Scatter plots:**  \n",
    "    - `plt.scatter()` shows how each feature relates to the target.  \n",
    "    - A stronger linear trend (e.g., **GrLivArea vs SalePrice**) suggests that **linear regression** may fit well. \n",
    "- **Subplots setup:**  \n",
    "    - `plt.subplot(131)` creates the **first** plot in a row of 3.  \n",
    "    - `132`, `133` specify the **second** and **third** plots.  \n",
    "- **Other details:**  \n",
    "    - `alpha=0.6` makes points **semi-transparent**, so overlaps are easier to see.  \n",
    "    - `plt.tight_layout()` prevents titles/labels from overlapping.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features \n",
    "X = df[[\"GrLivArea\", \"BedroomAbvGr\", \"GarageCars\"]]\n",
    "y = df[\"SalePrice\"] / 1000  # in thousands for readability\n",
    "\n",
    "# Scatter plots using subplot(131), (132), (133)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[\"GrLivArea\"], y, alpha=0.6)\n",
    "plt.title(\"GrLivArea vs SalePrice\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice ($1000s)\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[\"BedroomAbvGr\"], y, alpha=0.6)\n",
    "plt.title(\"BedroomAbvGr vs SalePrice\")\n",
    "plt.xlabel(\"BedroomAbvGr\")\n",
    "plt.ylabel(\"SalePrice ($1000s)\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[\"GarageCars\"], y, alpha=0.6)\n",
    "plt.title(\"GarageCars vs SalePrice\")\n",
    "plt.xlabel(\"GarageCars\")\n",
    "plt.ylabel(\"SalePrice ($1000s)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d3161",
   "metadata": {},
   "source": [
    "## 1.3 Simple Linear Regression  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Data splitting**  \n",
    "  - `train_test_split()` divides the dataset into training and test sets.  \n",
    "  - Training set: used to fit (learn) the model.  \n",
    "  - Test set: used to evaluate how well the model generalizes to unseen data.  \n",
    "\n",
    "- **Model training**  \n",
    "  - `LinearRegression().fit()` finds the best-fit line by minimizing the sum of squared errors.  \n",
    "  - `model.coef_` = slope of the line (effect of the feature on price).  \n",
    "  - `model.intercept_` = intercept (baseline price when the feature = 0).  \n",
    "\n",
    "- **Evaluation metrics**  \n",
    "  - `mean_squared_error()` ‚Üí measures prediction error (lower is better).  \n",
    "  - `r2_score()` ‚Üí measures how much variance in the target is explained by the model (closer to 1 is better).  \n",
    "\n",
    "- **Train vs Test comparison**  \n",
    "  - Training performance shows how well the model fits the data it learned from.  \n",
    "  - Test performance shows how well it generalizes to new data.  \n",
    "  - Always compare both to detect **underfitting** (poor train + test) or **overfitting** (good train, poor test).  \n",
    "\n",
    "- **Visualization**  \n",
    "  - Scatter plot: predicted vs. observed sale prices on the test set.  \n",
    "  - The red dashed line represents a perfect prediction line (`y = x`).  \n",
    "  - The closer the points are to this line, the better the model performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd428db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X1 = X[[\"GrLivArea\"]]  # single feature\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X1_train, y_train)\n",
    "\n",
    "y_train_pred = model1.predict(X1_train)\n",
    "y_test_pred = model1.predict(X1_test)\n",
    "\n",
    "print(\"Coefficient:\", model1.coef_[0])\n",
    "print(\"Intercept:\", model1.intercept_)\n",
    "print(\"Train RMSE:\", mean_squared_error(y_train, y_train_pred, squared=False))\n",
    "print(\"Test RMSE:\", mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Train R¬≤:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test R¬≤:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Observed Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Simple Linear Regression (GrLivArea)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6da09c",
   "metadata": {},
   "source": [
    "## 1.4 Multiple Linear Regression (with Scaling)  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Why multiple regression?**  \n",
    "  - Instead of using only one feature (like `GrLivArea`), we now include **several predictors** to improve accuracy.  \n",
    "  - Each feature contributes to predicting `SalePrice` while controlling for the others.  \n",
    "\n",
    "- **Feature scaling**  \n",
    "  - `StandardScaler()` standardizes features so they have mean = 0 and standard deviation = 1.  \n",
    "  - This is important when predictors are measured on very different scales (e.g., `GrLivArea` in square feet vs. `GarageCars` in counts).  \n",
    "  - Scaling makes training more stable and ensures that features are treated fairly.  \n",
    "\n",
    "- **Pipeline**  \n",
    "  - `make_pipeline()` chains preprocessing steps (scaling) and model training into one object.  \n",
    "  - This ensures scaling is always applied consistently during both training and testing.  \n",
    "\n",
    "- **Evaluation**  \n",
    "  - `RMSE` (Root Mean Squared Error): measures average prediction error in the same units as the target (thousands of dollars here).  \n",
    "  - `R¬≤`: proportion of variance in house prices explained by the model (closer to 1 = better).  \n",
    "  - Always check both **training** and **test** performance.  \n",
    "\n",
    "- **Visualization**  \n",
    "  - Plot **Predicted vs. Observed** prices for the test set.  \n",
    "  - The red dashed line represents perfect predictions (`y = x`).  \n",
    "  - A good model has points close to this line, showing accurate predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132baa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model2 = make_pipeline(StandardScaler(), LinearRegression())\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model2.predict(X_train)\n",
    "y_test_pred = model2.predict(X_test)\n",
    "\n",
    "print(\"Training RMSE:\", mean_squared_error(y_train, y_train_pred, squared=False))\n",
    "print(\"Training R¬≤:\", r2_score(y_train, y_train_pred))\n",
    "print(\"Test RMSE:\", mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Test R¬≤:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n",
    "plt.xlabel(\"Observed Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Multiple Linear Regression (Scaled Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a118c",
   "metadata": {},
   "source": [
    "## 1.5 Polynomial Regression (No Regularization)  \n",
    "### 1.5.1 Polynomial Regression ‚Äî Single Feature  \n",
    "üí° **Tips**  \n",
    "\n",
    "- **PolynomialFeatures:**  \n",
    "    - `PolynomialFeatures(d)` expands a feature into higher-order terms.  \n",
    "    - Example (degree=3):  \n",
    "      - Input: \\(x\\)  \n",
    "      - Output: \\([1, x, x^2, x^3]\\)  \n",
    "    - Here we only use **GrLivArea** and expand it into polynomial terms.  \n",
    "\n",
    "- **Why polynomial regression?**  \n",
    "    - A simple linear model (straight line) may miss nonlinear patterns.  \n",
    "    - Polynomial regression allows the model to \"bend\" and capture curved relationships.  \n",
    "    \n",
    "- **Overfitting risk:**  \n",
    "    - Training error **always decreases** as degree increases (the model fits the training data better).  \n",
    "    - Test error may start **increasing** at higher degrees ‚Üí this is overfitting.  \n",
    "\n",
    "- **Visualization strategy:**  \n",
    "    - 1. Plot polynomial fits for **degrees 1, 2, 5, and 10**.  \n",
    "       - Compare how the curve shape changes as degree increases.  \n",
    "    - 2. Plot **train vs. test error** across degrees (e.g., 1‚Äì13).  \n",
    "       - Use `plt.yscale(\"log\")` since MSE values can differ by orders of magnitude.  \n",
    "- 3. **Smooth grid for plotting**  \n",
    "     - Instead of plotting predictions only at training data points, create a **smooth grid** of values for the feature:  \n",
    "       ```python\n",
    "       X_plot = np.linspace(X1.min(), X1.max(), 200).reshape(-1, 1)\n",
    "       ```  \n",
    "     - `np.linspace(..., 200)` generates **200 evenly spaced values** across the feature‚Äôs range.  \n",
    "     - This makes the prediction curve **smooth and continuous**, which is much clearer than plotting only at raw data points.  \n",
    "     \n",
    "- **Loop options:**  \n",
    "    - **Using index `i`:**  \n",
    "    \n",
    "```python\n",
    "      for i in range(len(degrees_to_plot)):\n",
    "          d = degrees_to_plot[i]\n",
    "          plt.subplot(2, 2, i+1)  # subplot index from i is used because subplot indices start at 1, not 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8663ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Choose a few polynomial degrees to visualize\n",
    "degrees_to_plot = [1, 2, 5, 10]\n",
    "\n",
    "# Create a smooth grid for plotting\n",
    "X_plot = np.linspace(X1.min(), X1.max(), 200).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "i = 0\n",
    "for i in range(len(degrees_to_plot)):\n",
    "    d = degrees_to_plot[i]\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X1_train, y_train)\n",
    "    y_plot = model.predict(X_plot)\n",
    "\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(X1_train, y_train, color=\"blue\", alpha=0.5, label=\"Train data\")\n",
    "    plt.plot(X_plot, y_plot, color=\"red\", linewidth=2, label=f\"Degree {d}\")\n",
    "    plt.xlabel(\"GrLivArea\")\n",
    "    plt.ylabel(\"SalePrice\")\n",
    "    plt.title(f\"Polynomial Fit (Degree {d})\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a355541",
   "metadata": {},
   "source": [
    "üí° **Tips for Training vs. Test Error Curve**  \n",
    "\n",
    "**Looping through degrees**  \n",
    "- `degrees = range(1, 14)` ‚Üí tests polynomial degrees from 1 (linear) to 13 (very flexible).  \n",
    "- Inside the loop, for each degree:  \n",
    "  - Expand features using `PolynomialFeatures(d)`.  \n",
    "  - Standardize with `StandardScaler()` (important for stability).  \n",
    "  - Fit a `LinearRegression()` model.  \n",
    "\n",
    "**Error calculation**  \n",
    "- Predict on both training and test sets.  \n",
    "- Compute **Mean Squared Error (MSE)** for train and test predictions.  \n",
    "- Append results to `train_errors` and `test_errors` lists.  \n",
    "\n",
    "**Plotting the errors**  \n",
    "- `plt.plot(degrees, train_errors, label=\"Train MSE\")` ‚Üí shows how training error changes with model complexity.  \n",
    "- `plt.plot(degrees, test_errors, label=\"Test MSE\")` ‚Üí shows generalization error.  \n",
    "- `plt.yscale(\"log\")` ‚Üí useful because MSE can vary by orders of magnitude.  \n",
    "\n",
    "**Interpreting the curves**  \n",
    "- Training error **always decreases** as degree increases (model fits training data better).  \n",
    "- Test error decreases at first but then **increases at higher degrees** (overfitting).  \n",
    "- The gap between train and test error reveals the **bias‚Äìvariance trade-off**:  \n",
    "  - Small gap, both errors high ‚Üí **underfitting**.  \n",
    "  - Large gap, test error much higher ‚Üí **overfitting**.  \n",
    "\n",
    "**Key takeaway**  \n",
    "- This plot is a diagnostic tool:  \n",
    "  - Identify the polynomial degree where test error is lowest.  \n",
    "  - Avoid models with too high a degree (overfit) or too low (underfit).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 14)\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X1_train, y_train)\n",
    "    y_train_pred = model.predict(X1_train)\n",
    "    y_test_pred = model.predict(X1_test)\n",
    "\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "plt.plot(degrees, train_errors, label=\"Train MSE\")\n",
    "plt.plot(degrees, test_errors, label=\"Test MSE\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Polynomial Regression (GrLivArea, No Regularization)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a999f",
   "metadata": {},
   "source": [
    "### 1.5.2 Polynomial Regression ‚Äî Multiple Features  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Polynomial expansion with multiple predictors**  \n",
    "  - `PolynomialFeatures(d)` also works when we have multiple input features.  \n",
    "  - Example (degree=2, with features $x_1, x_2$):  \n",
    "    - Generates: $ [1, x_1, x_2, x_1^2, x_2^2, x_1x_2] $  \n",
    "  - For 3 features (`GrLivArea`, `BedroomAbvGr`, `GarageCars`), expansion adds squared terms and interaction terms.  \n",
    "\n",
    "- **Feature explosion**  \n",
    "  - As the degree increases, the number of features grows rapidly.  \n",
    "  - This can cause **multicollinearity** (redundant features) and overfitting.  \n",
    "\n",
    "- **Error analysis**  \n",
    "  - Like with the single feature case, training error decreases with degree.  \n",
    "  - Test error may increase at higher degrees if the model is too complex.  \n",
    "\n",
    "- **Visualization**  \n",
    "  - Plot training vs. test MSE for degrees 1‚Äì5.  \n",
    "  - Use a **logarithmic y-axis** (`plt.yscale(\"log\")`) for better readability across large error differences.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaec3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 6)\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "plt.plot(degrees, train_errors, label=\"Train MSE\")\n",
    "plt.plot(degrees, test_errors, label=\"Test MSE\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Polynomial Regression (Three Features, No Regularization)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f989eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d6c25b",
   "metadata": {},
   "source": [
    "## 1.6 Ridge Regularization  \n",
    "\n",
    "üí° **Conceptual Overview**  \n",
    "\n",
    "- **Why Ridge?**  \n",
    "  - Polynomial regression with high degrees can be very flexible ‚Üí risk of **overfitting**.  \n",
    "  - Overfitting symptoms:  \n",
    "    - Training error ‚Üí very low.  \n",
    "    - Test error ‚Üí high.  \n",
    "\n",
    "- **Ridge regression (L2 regularization)**  \n",
    "  - Adds a penalty term to the cost function:  \n",
    "    $\n",
    "    J(w) = \\frac{1}{2m} \\sum_{i=1}^m \\big(y^{(i)} - \\hat{y}^{(i)}\\big)^2 + \\alpha \\sum_{j=1}^n w_j^2\n",
    "    $  \n",
    "  - The penalty discourages large coefficients.  \n",
    "\n",
    "- **Effect of Œ± (alpha)**  \n",
    "  - Œ± = 0 ‚Üí equivalent to ordinary least squares.  \n",
    "  - Small Œ± ‚Üí weak penalty, model close to unregularized regression.  \n",
    "  - Large Œ± ‚Üí strong penalty, coefficients shrink more, smoother model.  \n",
    "\n",
    "- **Key points**  \n",
    "  - The penalty term $\\sum w_j^2$ is **quadratic** (a smooth parabola).  \n",
    "  - During optimization, coefficients are pushed to become smaller.  \n",
    "  - Ridge keeps all predictors but reduces their influence. \n",
    "  - Improves stability and generalization.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.1 Ridge Regularization ‚Äì Train Error Curves  \n",
    "üí° **Tips**  \n",
    "\n",
    "**Setup:**  \n",
    "- Compare **no regularization** vs. Ridge with:  \n",
    "  - Weak regularization: $\\alpha = 0.1$  \n",
    "  - Strong regularization: $\\alpha = 100$  \n",
    "- Polynomial degrees: from 1 to 5.  \n",
    "- Errors are tracked separately for training and test sets.  \n",
    "\n",
    "**Loop for each Œ± value:**  \n",
    "- Initialize empty lists:  \n",
    "  - `training_errors_alpha_*` ‚Üí training MSE for all degrees.  \n",
    "  - `test_errors_alpha_*` ‚Üí test MSE for all degrees.  \n",
    "- For each polynomial degree:  \n",
    "  1. Expand features with `PolynomialFeatures(d)`.  \n",
    "  2. Standardize with `StandardScaler()` (important for Ridge).  \n",
    "  3. Fit `Ridge(alpha=...)` on training data.  \n",
    "  4. Predict on both training and test sets.  \n",
    "  5. Compute mean squared error (MSE).  \n",
    "  6. Append errors to the lists.  \n",
    "\n",
    "**Plotting strategy:**  \n",
    "- **Dashed gray line** = baseline (no regularization).  \n",
    "- **Colored solid lines** = Ridge regression curves (e.g., Œ±=0.1 and Œ±=100 in different colors).  \n",
    "- Plot training errors vs. polynomial degree for each Œ± separately (or together).  \n",
    "\n",
    "**Key takeaway:**  \n",
    "- Ridge **raises training error** compared to no regularization (since coefficients are shrunk).  \n",
    "- As Œ± increases, the model becomes **simpler and smoother** ‚Üí fits the training data less perfectly.  \n",
    "- The trade-off: weaker training fit but **potentially lower test error** (better generalization) at higher degrees.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "degrees = range(1, 6)\n",
    "alpha_weak = 0.1\n",
    "training_errors_alpha_weak = []\n",
    "test_errors_alpha_weak = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        Ridge(alpha=alpha_weak)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    training_errors_alpha_weak.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors_alpha_weak.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Plot training error curves\n",
    "plt.plot(degrees, train_errors, label=\"Train MSE (No Reg)\", linestyle=\"--\", color=\"gray\")\n",
    "plt.plot(degrees, training_errors_alpha_weak, label=f\"Train MSE (Ridge Œ±={alpha_weak})\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Effect of Ridge Regularization on Train Error\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33308de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 6)\n",
    "alpha_strong = 100\n",
    "training_errors_alpha_strong = []\n",
    "test_errors_alpha_strong = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        Ridge(alpha=alpha_strong)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    training_errors_alpha_strong.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors_alpha_strong.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Plot training error curves\n",
    "plt.plot(degrees, train_errors, label=\"Train MSE (No Reg)\", linestyle=\"--\", color=\"gray\")\n",
    "plt.plot(degrees, training_errors_alpha_strong, label=f\"Train MSE (Ridge Œ±={alpha_strong})\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Effect of Ridge Regularization on Train Error\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b331265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training error curves\n",
    "plt.plot(degrees, train_errors, label=\"Train MSE (No Reg)\", linestyle=\"--\", color=\"gray\")\n",
    "plt.plot(degrees, training_errors_alpha_weak, label=f\"Train MSE (Ridge Œ±={alpha_weak})\")\n",
    "plt.plot(degrees, training_errors_alpha_strong, label=f\"Train MSE (Ridge Œ±={alpha_strong})\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Effect of Ridge Regularization on Train Error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe8ae7",
   "metadata": {},
   "source": [
    "### 1.6.2 Ridge Regularization ‚Äì Test Error Curves  \n",
    "üí° **Tips:**  \n",
    "\n",
    "**Goal:**  \n",
    "- Compare how **test error** changes with polynomial degree under three conditions:  \n",
    "  1. No regularization (baseline).  \n",
    "  2. Ridge with \\(\\alpha = 0.1\\) (weak penalty).  \n",
    "  3. Ridge with \\(\\alpha = 100\\) (strong penalty).  \n",
    "\n",
    "**Plotting:**  \n",
    "- `plt.plot(degrees, test_errors, ...)` ‚Üí baseline test error (no regularization), dashed gray line.  \n",
    "- `plt.plot(degrees, test_errors_alpha_weak, ...)` ‚Üí Ridge with weak penalty.  \n",
    "- `plt.plot(degrees, test_errors_alpha_strong, ...)` ‚Üí Ridge with strong penalty.  \n",
    "- Each curve shows how **generalization error** changes as model complexity (degree) increases.  \n",
    "\n",
    "**Interpretation:**  \n",
    "- **No regularization:**  \n",
    "  - Test error decreases at first but often **rises sharply at high degrees** (overfitting).  \n",
    "- **Weak Ridge ($\\alpha = 0.1$):**  \n",
    "  - Test error improves slightly vs. baseline but may still increase at high degrees.  \n",
    "- **Strong Ridge ($\\alpha = 100$):**  \n",
    "  - Test error curve is **flatter and more stable**, showing Ridge‚Äôs ability to reduce variance.  \n",
    "\n",
    "**Key takeaway:**  \n",
    "- Ridge may not always minimize test error, but it **stabilizes performance** across degrees.  \n",
    "- This demonstrates the **bias‚Äìvariance tradeoff**:  \n",
    "  - Larger Œ± ‚Üí higher bias (train error increases).  \n",
    "  - But variance decreases ‚Üí test error stabilizes, improving **generalization**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbe6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test error curves\n",
    "plt.plot(degrees, test_errors, label=\"Train MSE (No Reg)\", linestyle=\"--\", color=\"gray\")\n",
    "plt.plot(degrees, test_errors_alpha_weak, label=f\"Train MSE (Ridge Œ±={alpha_weak})\")\n",
    "plt.plot(degrees, test_errors_alpha_strong, label=f\"Train MSE (Ridge Œ±={alpha_strong})\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Effect of Ridge Regularization on Test Error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f958be2",
   "metadata": {},
   "source": [
    "## Further Reading: Lasso Regularization ‚Äì Conceptual Overview  \n",
    "\n",
    "**Concept**  \n",
    "\n",
    "- **Lasso Regression (L1 Regularization)** is another technique to control overfitting.  \n",
    "- Unlike Ridge (which penalizes squared coefficients), Lasso penalizes the **absolute values** of coefficients.  \n",
    "- This penalty has a unique property: it can shrink some coefficients **exactly to zero**, effectively removing less important features.  \n",
    "\n",
    "---\n",
    "\n",
    "### Lasso Cost Function  \n",
    "\n",
    "$\n",
    "J(w) = \\frac{1}{2m} \\sum_{i=1}^m \\Big( y^{(i)} - \\hat{y}^{(i)} \\Big)^2 + \\alpha \\sum_{j=1}^n |w_j|\n",
    "$  \n",
    "\n",
    "- First term = standard mean squared error.  \n",
    "- Second term = L1 penalty, proportional to the absolute value of coefficients.  \n",
    "- **Œ± (alpha)** controls regularization strength:  \n",
    "  - **Œ± = 0** ‚Üí equivalent to ordinary least squares.  \n",
    "  - **Small Œ±** ‚Üí weak penalty, most coefficients remain nonzero.  \n",
    "  - **Large Œ±** ‚Üí strong penalty, many coefficients shrink exactly to zero.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of Lasso  \n",
    "\n",
    "- The L1 penalty introduces **sharp corners at zero** in the optimization landscape.  \n",
    "- At these corners, the optimizer may set coefficients **exactly to zero**.  \n",
    "- This means Lasso can **perform feature selection**, keeping only the most important predictors.  \n",
    "- Like Ridge, Lasso reduces overfitting, but it also creates **sparse models** (simpler, easier to interpret).  \n",
    "\n",
    "---\n",
    "\n",
    "### Ridge vs. Lasso  \n",
    "\n",
    "- **Ridge (L2)** ‚Üí shrinks coefficients but keeps all features.  \n",
    "  - ‚ÄúShrink but keep everything.‚Äù  \n",
    "- **Lasso (L1)** ‚Üí shrinks and can eliminate features.  \n",
    "  - ‚ÄúShrink and throw away what‚Äôs not useful.‚Äù  \n",
    "  \n",
    "---\n",
    "### Example: Applying Lasso Regression  \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Polynomial degree for feature expansion\n",
    "degree = 3  \n",
    "\n",
    "# Build pipeline: polynomial expansion + scaling + Lasso regression\n",
    "lasso_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=True),\n",
    "    StandardScaler(),\n",
    "    Lasso(alpha=0.1, max_iter=5000)  # alpha controls regularization strength\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962d17b",
   "metadata": {},
   "source": [
    "# Part 2. Logistic Regression Tutorial with the Covertype Dataset  \n",
    "\n",
    "In this part, we will learn how to apply **logistic regression** for classification, using the **Covertype dataset** from scikit-learn.  \n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "- Understand the basics of **logistic regression** for classification problems.  \n",
    "- Implement **binary logistic regression** using one or more features.  \n",
    "- Extend logistic regression to handle **multiclass classification**.  \n",
    "- Use **polynomial feature expansion** to model nonlinear relationships, while recognizing the risk of **overfitting**. \n",
    "- Apply **regularization (Ridge / Lasso)** to improve generalization and compare their effects.  \n",
    "- Evaluate classification models using **accuracy** and **classification reports** (precision, recall, F1).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc027f95",
   "metadata": {},
   "source": [
    "## 2.1 Load Dataset  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Dataset source**  \n",
    "  - Loaded using `fetch_openml(\"covertype\", version=3, as_frame=True)` from the OpenML repository.  \n",
    "  - Setting `as_frame=True` returns the data as a **pandas DataFrame**, which is convenient for exploration and preprocessing.  \n",
    "  - Originally created by the **U.S. Forest Service** to classify forest cover types.  \n",
    "\n",
    "- **Dataset size**  \n",
    "  - Contains **581,012 rows √ó 54 features**.  \n",
    "  - Each row corresponds to a **30√ó30 meter patch of forest**.  \n",
    "  - Features include:  \n",
    "    - **Elevation** (meters)  \n",
    "    - **Aspect** (azimuthal direction, degrees)  \n",
    "    - **Slope** (steepness, degrees)  \n",
    "    - **Horizontal & vertical distances** to hydrology, roadways, and fire points  \n",
    "    - **Hillshade measures** at 9am, noon, and 3pm  \n",
    "    - **Wilderness_Area** indicators (binary one-hot encoded, 4 values)  \n",
    "    - **Soil_Type** indicators (binary one-hot encoded, 40 values)  \n",
    "\n",
    "- **Target variable**  \n",
    "  - `Cover_Type`: categorical label with **7 possible forest cover classes**:  \n",
    "    1. Spruce/Fir  \n",
    "    2. Lodgepole Pine  \n",
    "    3. Ponderosa Pine  \n",
    "    4. Cottonwood/Willow  \n",
    "    5. Aspen  \n",
    "    6. Douglas-fir  \n",
    "    7. Krummholz  \n",
    "\n",
    "- **Why subsetting?**  \n",
    "  - Training logistic regression (especially with polynomial features) on all **580k rows** would be too slow for classroom or notebook experiments.  \n",
    "  - Instead of taking the first chunk of rows, we now **sample 1000 rows from each class without replacement**:  \n",
    "    - Ensures the dataset is **balanced across all 7 classes**.  \n",
    "    - Keeps training and visualization **fast** while still reflecting all cover types.  \n",
    "  - In real-world machine learning, we would use the **full dataset** to train and evaluate models.  \n",
    "\n",
    "- **Feature and target variables**  \n",
    "  - `X`: the 54 predictor variables (environmental + categorical).  \n",
    "  - `y`: the target (`Cover_Type`), stored as integers.  \n",
    "\n",
    "- **Quick check**  \n",
    "  - `print(\"Original shape:\", X.shape)` ‚Üí dataset size before sampling.  \n",
    "  - `print(\"Sampled shape:\", X.shape)` ‚Üí confirms the balanced subset size.  \n",
    "  - `y.value_counts()` ‚Üí shows class distribution (should be 1000 per class).  \n",
    "  - `X.head()` ‚Üí displays the first few rows for inspection.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "cover = fetch_openml(name=\"covertype\", version=3, as_frame=True)\n",
    "\n",
    "X_cover = cover.data\n",
    "y_cover = cover.target.astype(int)\n",
    "del cover # delete the large dataset to release memory\n",
    "\n",
    "print(\"Original shape:\", X_cover.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sample_classes(X, y, classes, n_per_class=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Sample equal number of rows without replacement from specified classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix.\n",
    "    y : pd.Series\n",
    "        Target vector.\n",
    "    classes : list\n",
    "        List of class labels to sample (e.g., [3, 4]).\n",
    "    n_per_class : int, default=1000\n",
    "        Number of samples per class.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_sampled : pd.DataFrame\n",
    "        Subsampled features.\n",
    "    y_sampled : pd.Series\n",
    "        Subsampled targets.\n",
    "    \"\"\"\n",
    "    sampled_X, sampled_y = [], []\n",
    "    for cls in classes:\n",
    "        mask = (y == cls)\n",
    "        X_cls = X[mask]\n",
    "        y_cls = y[mask]\n",
    "        X_sub = X_cls.sample(n=n_per_class, replace=False, random_state=random_state)\n",
    "        y_sub = y_cls.loc[X_sub.index]\n",
    "        sampled_X.append(X_sub)\n",
    "        sampled_y.append(y_sub)\n",
    "    return pd.concat(sampled_X), pd.concat(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53cef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: sample 1000 rows from class 3 and 1000 from class 4\n",
    "X, y = sample_classes(X_cover, y_cover, classes=[1, 2, 3, 4, 5, 6, 7], n_per_class=1000)\n",
    "\n",
    "print(\"Sampled shape:\", X.shape)\n",
    "print(\"Class counts:\\n\", y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662cb41",
   "metadata": {},
   "source": [
    "## 2.2 Select and Visualize Features  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Feature types**  \n",
    "  - The Covertype dataset includes two kinds of features:  \n",
    "    - **Continuous (numeric):** e.g., `Elevation`, `Slope`, `Aspect`.  \n",
    "    - **Categorical indicators (0/1 flags):** e.g., `Wilderness_Area*`, `Soil_Type*`.  \n",
    "  - In this step, we focus on continuous features, since they are easier to visualize.  \n",
    "\n",
    "- **Feature distributions by class:**  \n",
    "    - `plt.boxplot()` shows how a feature‚Äôs values are distributed across cover types.  \n",
    "    - Example: comparing **Elevation** across cover types may reveal which forests occur at higher vs. lower altitudes.  \n",
    "- Boxplots are helpful for spotting both **differences** and **overlaps** among classes.  \n",
    "\n",
    "- **Subplots setup:**  \n",
    "    - `plt.subplot(131)`, `plt.subplot(132)`, `plt.subplot(133)` create a row of 3 side-by-side plots.  \n",
    "    - Each subplot displays one feature (`Elevation`, `Aspect`, or `Slope`).  \n",
    "    - `plt.tight_layout()` automatically adjusts spacing to avoid overlapping titles and labels.  \n",
    "\n",
    "- **Takeaway**  \n",
    "  - These plots provide a **first look** at how the target classes differ in terms of continuous features.  \n",
    "  - Later, logistic regression will try to exploit these differences to separate classes.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "features_to_plot = [\"Elevation\", \"Aspect\", \"Slope\"]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Subplot 1: Elevation\n",
    "plt.subplot(131)\n",
    "data = [X[y == cls][\"Elevation\"] for cls in np.unique(y)]\n",
    "plt.boxplot(data, labels=np.unique(y))\n",
    "plt.title(\"Elevation vs Cover Type\")\n",
    "plt.xlabel(\"Cover Type\")\n",
    "plt.ylabel(\"Elevation\")\n",
    "\n",
    "# Subplot 2: Aspect\n",
    "plt.subplot(132)\n",
    "data = [X[y == cls][\"Aspect\"] for cls in np.unique(y)]\n",
    "plt.boxplot(data, labels=np.unique(y))\n",
    "plt.title(\"Aspect vs Cover Type\")\n",
    "plt.xlabel(\"Cover Type\")\n",
    "plt.ylabel(\"Aspect\")\n",
    "\n",
    "# Subplot 3: Slope\n",
    "plt.subplot(133)\n",
    "data = [X[y == cls][\"Slope\"] for cls in np.unique(y)]\n",
    "plt.boxplot(data, labels=np.unique(y))\n",
    "plt.title(\"Slope vs Cover Type\")\n",
    "plt.xlabel(\"Cover Type\")\n",
    "plt.ylabel(\"Slope\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd76ed1",
   "metadata": {},
   "source": [
    "### 2.3 Binary Logistic Regression (One Feature)  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Why binary classification?**  \n",
    "  - The Covertype dataset has 7 classes, but logistic regression is easiest to understand in the **binary case**.  \n",
    "  - Here, we focus only on two classes (e.g., class 3 vs. class 6) to demonstrate the basics.  \n",
    "\n",
    "- **Feature selection**  \n",
    "  - We use only **one feature**: `Elevation`.  \n",
    "  - This makes it simple to visualize and interpret how logistic regression separates two classes.  \n",
    "\n",
    "- **Data splitting**  \n",
    "  - `train_test_split()` splits the data into training (70%) and test (30%) sets.  \n",
    "  - Training set ‚Üí used to fit the model.  \n",
    "  - Test set ‚Üí used to evaluate generalization performance.  \n",
    "\n",
    "- **Model training**  \n",
    "  - `LogisticRegression(max_iter=2000, n_jobs=-1)`  \n",
    "    - Creates the logistic regression model.  \n",
    "    - `max_iter=2000`: allows more iterations so the solver converges reliably.  \n",
    "    - `n_jobs=-1`: uses all CPU cores for faster computation.  \n",
    "  - **Penalty behavior**:  \n",
    "    - By default, scikit-learn applies **L2 penalty** (Ridge-style regularization).  \n",
    "    - You can override with `penalty=\"none\"` for no regularization (pure logistic regression).  \n",
    "    - With certain solvers (`liblinear`, `saga`), you can also use:  \n",
    "      - `penalty=\"l1\"` ‚Üí Lasso-style regularization.  \n",
    "      - `penalty=\"elasticnet\"` ‚Üí mix of L1 and L2.  \n",
    "  - `.fit(X_train, y_train)`  \n",
    "    - Fits the model to the training data.  \n",
    "    - Learns the best parameters (weights for the feature and a bias term).  \n",
    "    - Unlike linear regression, logistic regression outputs **probabilities**, which are then thresholded (default 0.5) to assign class labels.  \n",
    "\n",
    "- **Evaluation metrics**  \n",
    "  - `accuracy_score`: overall proportion of correctly classified samples.  \n",
    "  - `classification_report`: shows **precision, recall, and F1-score** for each class.  \n",
    "    - **Precision** = out of predicted positives, how many are correct.  \n",
    "    - **Recall** = out of actual positives, how many are detected.  \n",
    "    - **F1-score** = harmonic mean of precision and recall (balances both).  \n",
    "\n",
    "- **Key takeaway**  \n",
    "  - Using just one feature, the model is very simple, but it clearly demonstrates how logistic regression works.  \n",
    "  - Later, we will extend to multiple features and multiclass classification for better performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Subset: classes 3 and 6 only\n",
    "mask = y.isin([3, 6])\n",
    "X_bin, y_bin = X[mask], y[mask]\n",
    "\n",
    "# One feature\n",
    "X_small = X_bin[[\"Elevation\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_bin, test_size=0.3, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=2000, n_jobs=-1, penalty = \"none\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68864493",
   "metadata": {},
   "source": [
    "### Interpreting Classification Report  \n",
    "\n",
    "**Class 3:**  \n",
    "- **Precision = 0.53** ‚Üí Of all samples predicted as class 3, 53% were correct.  \n",
    "- **Recall = 0.45** ‚Üí Out of the 302 true class 3 samples, only 45% were correctly identified.  \n",
    "- **F1 = 0.49** ‚Üí The balance of precision and recall; performance is modest.  \n",
    "- **Support = 302** ‚Üí Number of class 3 samples in the test set.  \n",
    "\n",
    "**Class 6:**  \n",
    "- **Precision = 0.52** ‚Üí Of all samples predicted as class 6, 52% were correct.  \n",
    "- **Recall = 0.60** ‚Üí Out of the 298 true class 6 samples, 60% were correctly identified.  \n",
    "- **F1 = 0.56** ‚Üí Better than class 3, mainly due to higher recall.  \n",
    "- **Support = 298** ‚Üí Number of class 6 samples in the test set.  \n",
    "\n",
    "**Overall accuracy = 0.52**  \n",
    "- The model correctly classified about **52%** of test samples ‚Äî only slightly above random guessing (50% for two balanced classes).  \n",
    "\n",
    "**Macro avg (equal weight for each class):**  \n",
    "- Precision = 0.52, Recall = 0.52, F1 = 0.52.  \n",
    "- Shows the model performs almost the same across both classes.  \n",
    "\n",
    "**Weighted avg (accounts for class sizes):**  \n",
    "- Nearly identical to macro average, since classes are roughly balanced.  \n",
    "\n",
    "**Key Takeaways**\n",
    "- With just **one feature (Elevation)**, logistic regression performs **only slightly better than chance 50%**.  \n",
    "- The model struggles to separate classes 3 and 6, as their elevation ranges overlap heavily.  \n",
    "- Class 6 is predicted with slightly better recall, while class 3 is harder to identify correctly.  \n",
    "- This example highlights the **limitations of single-feature models** ‚Äî adding more features will likely improve performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d16948",
   "metadata": {},
   "source": [
    "## 2.4 Binary Logistic Regression (Three Features)  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Why extend to multiple features?**  \n",
    "  - With only one feature (`Elevation`), the model can only separate classes along a single axis.  \n",
    "  - Adding **more features** (`Slope` and `Aspect`) gives logistic regression more information to distinguish between classes.  \n",
    "  - This allows the model to create more complex **linear decision boundaries** in higher dimensions.  \n",
    "\n",
    "- **Feature selection**  \n",
    "  - We now use three continuous features:  \n",
    "    - `Elevation`  \n",
    "    - `Slope`  \n",
    "    - `Aspect`  \n",
    "\n",
    "- **Data splitting**  \n",
    "  - The same procedure as before: `train_test_split()` splits into training and test sets.  \n",
    "  - Ensures that model performance is evaluated on unseen data.  \n",
    "\n",
    "- **Model training**  \n",
    "  - We reuse the same logistic regression model (`logreg.fit()`).  \n",
    "  - Logistic regression automatically extends to multiple features by learning a **weight** for each feature plus an intercept.  \n",
    "  - The decision boundary is now a **plane** in 3D space (instead of a line in 2D).  \n",
    "\n",
    "- **Evaluation metrics**  \n",
    "  - `accuracy_score`: percentage of correct predictions.  \n",
    "  - `classification_report`: provides **precision, recall, and F1-score** for each class.  \n",
    "  - Comparison to the one-feature model shows whether the extra features improve classification.  \n",
    "\n",
    "- **Key takeaway**  \n",
    "  - Logistic regression scales naturally from one to multiple features.  \n",
    "  - By including more features, we typically improve accuracy, as the model can better capture the differences between classes.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e49b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X_bin[[\"Elevation\", \"Slope\", \"Aspect\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_bin, test_size=0.3, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a104b0",
   "metadata": {},
   "source": [
    "### Interpreting Classification Report  \n",
    "\n",
    "**Class 3:**  \n",
    "- **Precision = 0.54** ‚Üí Of all samples predicted as class 3, 54% were correct.  \n",
    "- **Recall = 0.52** ‚Üí Out of the 302 true class 3 samples, 52% were correctly identified.  \n",
    "- **F1 = 0.53** ‚Üí Balanced precision/recall performance, but modest overall.  \n",
    "- **Support = 302** ‚Üí Number of class 3 samples in the test set.  \n",
    "\n",
    "**Class 6:**  \n",
    "- **Precision = 0.53** ‚Üí Of all samples predicted as class 6, 53% were correct.  \n",
    "- **Recall = 0.55** ‚Üí Out of the 298 true class 6 samples, 55% were correctly identified.  \n",
    "- **F1 = 0.54** ‚Üí Slightly better balance than class 3 due to higher recall.  \n",
    "- **Support = 298** ‚Üí Number of class 6 samples in the test set.  \n",
    "\n",
    "**Overall accuracy = 0.54**  \n",
    "- The model correctly classified about **54%** of test samples.  \n",
    "- This is only **slightly above random guessing (50%)**, so the separation between classes remains weak.  \n",
    "\n",
    "**Macro avg (equal weight per class):**  \n",
    "- Precision = 0.54, Recall = 0.54, F1 = 0.54.  \n",
    "- Indicates both classes perform about the same.  \n",
    "\n",
    "**Weighted avg (accounts for class sizes):**  \n",
    "- Nearly identical to macro avg, since classes are balanced (302 vs. 298).  \n",
    "\n",
    "**Key Takeaways**  \n",
    "- Adding **Slope** and **Aspect** did not significantly improve performance compared to using only Elevation (‚âà52%).  \n",
    "- The logistic regression model struggles because **classes 3 and 6 overlap heavily across these continuous features**.  \n",
    "- Both classes are predicted with similar (but weak) quality ‚Üí no major bias toward one class.  \n",
    "- This shows the **limits of linear logistic regression with few features**. To improve accuracy, the model may need **additional features** or **more flexible, higher-degree representations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460bc40",
   "metadata": {},
   "source": [
    "## 2.5 Polynomial Features (No Regularization)  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **Why polynomial features?**  \n",
    "  - `PolynomialFeatures(degree=d)` expands the input features by adding:  \n",
    "    - **Squared terms** (e.g., `Elevation¬≤`, `Slope¬≤`).  \n",
    "    - **Interaction terms** (e.g., `Slope √ó Aspect`).  \n",
    "  - This allows logistic regression (normally a **linear classifier**) to model **nonlinear decision boundaries**.  \n",
    "\n",
    "- **Flexibility vs. overfitting**  \n",
    "  - Increasing the polynomial degree makes the model more flexible.  \n",
    "  - **Training accuracy**: always improves (or stays the same).  \n",
    "  - **Test accuracy**: may decrease at higher degrees because the model starts fitting noise ‚Üí **overfitting**.  \n",
    "\n",
    "- **Pipeline setup**  \n",
    "  - `PolynomialFeatures(degree=d)`: generates polynomial and interaction terms.  \n",
    "  - `StandardScaler()`: standardizes features to mean = 0 and standard deviation = 1 (improves numerical stability).  \n",
    "  - `LogisticRegression(...)`: fits the model.  \n",
    "    - `max_iter=2000`: allows more optimization iterations to ensure convergence.  \n",
    "    - `solver=\"lbfgs\"`: quasi-Newton optimization method, efficient for medium- to large-sized datasets.  \n",
    "    - **Penalty behavior:**  \n",
    "      - By default, scikit-learn applies **L2 penalty** (Ridge-style regularization).  \n",
    "      - You can override with `penalty=\"none\"` for no regularization (pure logistic regression).  \n",
    "      - With certain solvers (`liblinear`, `saga`), you can also use `penalty=\"l1\"` (Lasso) or `penalty=\"elasticnet\"`.  \n",
    "\n",
    "- **Other solver options**  \n",
    "  - `\"liblinear\"` ‚Üí good for small datasets; supports L1 and L2.  \n",
    "  - `\"saga\"` ‚Üí scalable for large datasets; supports L1, L2, and elastic net.  \n",
    "  - `\"newton-cg\"`, `\"sag\"`, `\"lbfgs\"` ‚Üí efficient for large-scale L2-regularized models.  \n",
    "\n",
    "- **Why no learning rate parameter?**  \n",
    "  - Unlike SGD-based training, these solvers automatically choose step sizes (learning rates) internally using **line search or adaptive strategies**.  \n",
    "  - That‚Äôs why `LogisticRegression` does **not** expose a `learning_rate` parameter.  \n",
    "  - If you want explicit learning rate control, use `SGDClassifier(loss=\"log_loss\")`.  \n",
    "\n",
    "- **Looping over degrees**  \n",
    "  - Train the model for polynomial degrees 1‚Äì15.  \n",
    "  - Record both training and test accuracy.  \n",
    "\n",
    "- **Visualization**  \n",
    "  - `plt.plot(degrees, train_accs, ...)`: plots training accuracy curve.  \n",
    "  - `plt.plot(degrees, test_accs, ...)`: plots test accuracy curve.  \n",
    "  - Comparing the two curves reveals:  \n",
    "    - **Underfitting** ‚Üí both train/test accuracy low (small degree).  \n",
    "    - **Overfitting** ‚Üí train accuracy high, test accuracy lower (large degree).  \n",
    "\n",
    "- **Key takeaway**  \n",
    "  - Polynomial features greatly increase model complexity.  \n",
    "  - Without regularization (`penalty=\"none\"`), high-degree polynomials tend to overfit.  \n",
    "  - Logistic regression **by default includes L2 regularization** ‚Äî you must explicitly set `penalty=\"none\"` to turn it off.  \n",
    "  - Always compare training and test curves to spot the bias‚Äìvariance tradeoff.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "degrees = range(1, 16)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for d in degrees:\n",
    "    poly_model = make_pipeline(\n",
    "        PolynomialFeatures(degree=d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=2000, solver=\"lbfgs\", penalty=\"none\", n_jobs=-1)\n",
    "    )\n",
    "    poly_model.fit(X_train, y_train)\n",
    "    train_accs.append(accuracy_score(y_train, poly_model.predict(X_train)))\n",
    "    test_accs.append(accuracy_score(y_test, poly_model.predict(X_test)))\n",
    "    print(f\"degree={d}\")\n",
    "\n",
    "plt.plot(degrees, train_accs, marker=\"o\", label=\"Train Accuracy\")\n",
    "plt.plot(degrees, test_accs, marker=\"s\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Polynomial Degree (No Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69b91f",
   "metadata": {},
   "source": [
    "## 2.6 Regularization (L2 Penalty)  \n",
    "üí° **Tips**  \n",
    "\n",
    "- **Why regularization?**  \n",
    "    - Logistic regression with **polynomial features** can become too flexible and overfit the training data.  \n",
    "    - **L2 penalty (Ridge-style regularization)** helps control complexity by shrinking coefficients.  \n",
    "    - This reduces variance and makes the model generalize better.  \n",
    "\n",
    "- **The `C` parameter**  \n",
    "    - `C` is the **inverse of regularization strength**:  \n",
    "      - Small `C` (e.g., 0.1) ‚Üí **strong regularization** ‚Üí simpler model, higher bias, lower variance.  \n",
    "      - Large `C` (e.g., 100) ‚Üí **weak regularization** ‚Üí more flexible model, lower bias, higher variance (risk of overfitting).  \n",
    "    - `penalty=\"none\"` ‚Üí baseline with **no regularization**.  \n",
    "\n",
    "- **Experiment setup**  \n",
    "    - Test polynomial degrees **1‚Äì15**.  \n",
    "    - For each degree, fit logistic regression models with:  \n",
    "      - `C=100` (weak regularization).  \n",
    "      - `C=0.1` (strong regularization).  \n",
    "      - `penalty=\"none\"` (no regularization).  \n",
    "    - Record **training accuracy** and **test accuracy** for comparison.  \n",
    "\n",
    "- **Code structure**  \n",
    "    - `train_accs_*`, `test_accs_*` ‚Üí lists that store accuracies for each condition.  \n",
    "    - **Loop** over degrees ‚Üí fit models with different regularization.  \n",
    "    - Save results ‚Üí used later for plotting.  \n",
    "\n",
    "- **Plotting**  \n",
    "    - **Left subplot** ‚Üí Training accuracy vs. polynomial degree.  \n",
    "      - Shows how well models fit training data.  \n",
    "    - **Right subplot** ‚Üí Test accuracy vs. polynomial degree.  \n",
    "      - Shows how well models generalize to unseen data.  \n",
    "\n",
    "- **Key takeaway**  \n",
    "    - No regularization ‚Üí high training accuracy, but test accuracy drops (overfitting).  \n",
    "    - Strong regularization ‚Üí lower training accuracy, but test accuracy is **more stable** (better generalization).  \n",
    "    - Regularization balances the **bias‚Äìvariance trade-off**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79749f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 16)\n",
    "C_strong = 100\n",
    "train_accs_C_strong = []\n",
    "test_accs_C_strong = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=C_strong, penalty=\"l2\", n_jobs=-1)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accs_C_strong.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_accs_C_strong.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "    print(f\"degree={d}\")\n",
    "\n",
    "# Training accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(degrees, train_accs_C_strong, marker=\"o\", label=f\"C={C_strong}, small regularization\")\n",
    "plt.plot(degrees, train_accs, marker=\"o\", label=\"Train Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(degrees, test_accs_C_strong, marker=\"s\", label=f\"C={C_strong}, small regularization\")\n",
    "plt.plot(degrees, test_accs, marker=\"s\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9e9e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degrees = range(1, 16)\n",
    "C_weak = 0.1\n",
    "train_accs_C_weak = []\n",
    "test_accs_C_weak = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=d, include_bias=True),\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=C_weak, penalty=\"l2\")\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accs_C_weak.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_accs_C_weak.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "    print(f\"degree={d}\")\n",
    "\n",
    "# Training accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(degrees, train_accs_C_weak, marker=\"o\", label=f\"C={C_weak}, large regularization\")\n",
    "plt.plot(degrees, train_accs, marker=\"o\", label=\"Train Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(degrees, test_accs_C_weak, marker=\"s\", label=f\"C={C_weak}, large regularization\")\n",
    "plt.plot(degrees, test_accs, marker=\"s\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef8428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(degrees, train_accs_C_strong, marker=\"o\", label=f\"C={C_strong}, small regularization\")\n",
    "plt.plot(degrees, train_accs_C_weak, marker=\"o\", label=f\"C={C_weak}, large regularization\")\n",
    "plt.plot(degrees, train_accs, marker=\"o\", label=\"Train Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(degrees, test_accs_C_strong, marker=\"s\", label=f\"C={C_strong}, small regularization\")\n",
    "plt.plot(degrees, test_accs_C_weak, marker=\"s\", label=f\"C={C_weak}, large regularization\")\n",
    "plt.plot(degrees, test_accs, marker=\"s\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy vs Degree\\n (With and Without Regularization)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b422be",
   "metadata": {},
   "source": [
    "## 2.7 Extend to Three Classes  \n",
    "\n",
    "üí° **Tips**  \n",
    "\n",
    "- **From binary to multiclass**  \n",
    "  - Logistic regression naturally supports **multiclass classification**.  \n",
    "  - By setting `multi_class=\"multinomial\"`, the model uses the [**softmax function**](https://www.geeksforgeeks.org/deep-learning/what-is-softmax-classifier/) to assign probabilities across all classes.  \n",
    "  - This is more appropriate than training multiple one-vs-rest models when classes are not just binary.  \n",
    "\n",
    "- **Dataset selection**  \n",
    "  - Instead of just two classes (e.g., 3 vs. 6), we now include **three classes** (3, 5, and 6).  \n",
    "  - This allows us to see how logistic regression handles more than two categories at once.  \n",
    "\n",
    "- **Model setup**  \n",
    "  - `LogisticRegression(max_iter=2000, multi_class=\"multinomial\", solver=\"lbfgs\")`:  \n",
    "    - `multi_class=\"multinomial\"` ‚Üí enables softmax-based multiclass classification.  \n",
    "    - `solver=\"lbfgs\"` ‚Üí efficient optimization method that supports multinomial logistic regression.  \n",
    "    - `max_iter=2000` ‚Üí ensures the model has enough iterations to converge.  \n",
    "    **By default**  \n",
    "        - `penalty=\"l2\"` ‚Üí Ridge-style regularization.  \n",
    "        - `C=1.0` ‚Üí inverse of regularization strength.  \n",
    "          - Smaller `C` = stronger penalty (more shrinkage).  \n",
    "          - Larger `C` = weaker penalty (closer to no regularization).  \n",
    "\n",
    "- **Evaluation metrics**  \n",
    "  - `accuracy_score`: overall proportion of correctly classified samples.  \n",
    "  - `classification_report`: provides **precision, recall, and F1-score** for each of the three classes.  \n",
    "  - These metrics help evaluate if the model is biased toward certain classes or performs evenly across them.  \n",
    "\n",
    "- **Key takeaway**  \n",
    "  - Logistic regression extends naturally to handle multiple classes.  \n",
    "  - By using the multinomial setting, the model can learn decision boundaries that separate all classes simultaneously.  \n",
    "  - Performance depends on how distinct the feature distributions are for each class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89409a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y.isin([3, 5, 6])\n",
    "X_multi, y_multi = X[mask], y[mask]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)\n",
    "\n",
    "logreg_multi = LogisticRegression(max_iter=2000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "logreg_multi.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_multi.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc405aea",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "\n",
    "- Begin with a **new Jupyter Notebook** containing only your **code, figures, and explanations**.  \n",
    "- Save and submit your notebook with the following filename format:  \n",
    "\n",
    "  **Lastname_Firstname_NetID_Assignment2.ipynb**  \n",
    "\n",
    "\n",
    "# Part 1: Linear Regression with Ames Housing Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Multiple Linear Regression (‚â• 2 features)**  (1 pt)\n",
    "- Split the dataset into **training (70%)** and **test (30%)** sets.  \n",
    "- Start with one key feature (e.g., `GrLivArea`) and add three others (e.g., `GarageCars`, `LotArea`, `TotRmsAbvGrd`).  \n",
    "- Use a pipeline with `StandardScaler()` and `LinearRegression()`.  \n",
    "- Fit the model and report:  \n",
    "  - Training RMSE and R¬≤  \n",
    "  - Test RMSE and R¬≤  \n",
    "- Plot **Predicted vs Observed** for the test set.  \n",
    "- Compare your results with earlier sections (1.3 Simple Linear Regression and 1.4 Multiple Regression).  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Polynomial Regression (no regularization)**  (1 pt)\n",
    "- Fit polynomial regression models with degrees **1 through 10** using the selected features.  \n",
    "- Record **training and test MSE** at each degree.  \n",
    "- Plot both training error vs degree and test error vs degree side by side\n",
    "- Discuss: At which polynomial degree does **overfitting** begin to appear?  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Ridge Regularization**  (1 pt)\n",
    "- Repeat polynomial regression, but now add Ridge regularization.  \n",
    "- Train models with **Œ± = 0.1, Œ± = 1, and Œ± = 100**.  \n",
    "- Plot training error vs degree and test error vs degree, comparing with the no-regularization baseline.  \n",
    "- Discuss how Ridge regularization helps reduce **overfitting** in high-degree polynomial models.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92a458",
   "metadata": {},
   "source": [
    "# Part 2: Logistic Regression with Covertype Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Load Data and Visualization** (1 pt)\n",
    "- Select **two classes other than 3 vs 6** (e.g., 3 vs 4, or 2 vs 5).  \n",
    "- Choose one numerical feature (e.g., `Elevation`).  \n",
    "- Plot histograms for this feature, showing both classes in the same figure.  \n",
    "- Fit a `LogisticRegression` model using this single feature.  \n",
    "- Report:  \n",
    "  - Training accuracy and test accuracy  \n",
    "  - Classification report (precision, recall, F1-score)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Logistic Regression with Multiple Features** (1 pt)\n",
    "- Select **three or more numerical features** (e.g., `Elevation`, `Slope`, `Aspect`).  \n",
    "- Fit a `LogisticRegression` model.  \n",
    "- Report:  \n",
    "  - Training accuracy and test accuracy  \n",
    "  - Classification report  \n",
    "- Compare the performance with Problem 1.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Polynomial Features and Ridge Regularization** (1 pt)\n",
    "- Choose a relatively **high polynomial degree** (e.g., ‚â• 5).  \n",
    "- Select two **C values** (e.g., 0.01, 0.1, 1, 10, or 100) ‚Äî remember:  \n",
    "  - **Small C ‚Üí strong regularization**  \n",
    "  - **Large C ‚Üí weak regularization**  \n",
    "- Fit three logistic regression models and compare:  \n",
    "  - **No penalty** (`penalty=\"none\"`)  \n",
    "  - **L2 penalty (Ridge)** (`penalty=\"l2\"`, default)  \n",
    "- Report:  \n",
    "  - Training accuracy and test accuracy  \n",
    "  - Classification report (precision, recall, F1-score)  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Bonus: Lasso Regularization** (Bonus 1 pts)  \n",
    "\n",
    "- Choose a relatively **high polynomial degree** (e.g., ‚â• 5).  \n",
    "- Select several **C values** (e.g., 0.01, 0.1, 1, or 10) ‚Äî remember:  \n",
    "  - **Small C ‚Üí strong regularization**  \n",
    "  - **Large C ‚Üí weak regularization**  \n",
    "- Fit three logistic regression models and compare:  \n",
    "  - **No penalty** (`penalty=\"none\"`)  \n",
    "  - **L1 penalty (Lasso)** (`penalty=\"l1\"`, requires solver `\"liblinear\"` or `\"saga\"`)  \n",
    "- Report **training and test accuracy** for each case.  \n",
    "- Discuss:  \n",
    "  - How are your results different from Probelm 3?  \n",
    "  - How does the choice of C affect the results?  \n",
    "\n",
    "\n",
    "üí° **Tips:**  \n",
    "- `LogisticRegression` uses **L2 penalty (Ridge)** by default.  \n",
    "- To use **L1 penalty (Lasso)**, set `penalty=\"l1\"` and `solver=\"liblinear\"` or `solver=\"saga\"` (since not all solvers support L1).  \n",
    "- To remove regularization, set `penalty=\"none\"` (available with `solver=\"lbfgs\"` or `solver=\"saga\"`).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e10762",
   "metadata": {},
   "source": [
    "# Part 3: Flux Tower Data Regression (4 pts)\n",
    "\n",
    "Apply what you have learned to build regression models on **daily Flux Tower data (2005‚Äì2021) in Assignment 1**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Instructions**\n",
    "1. Split the dataset:  \n",
    "   - Train on **2005‚Äì2020**  \n",
    "   - Predict **2021**  \n",
    "\n",
    "2. Start simple:  \n",
    "   - Start with the **single feature that you think is most related** with photosynthesis.  \n",
    "   - Fit a simple linear regression model using this feature.  \n",
    "\n",
    "3. Add complexity:  \n",
    "   - Gradually include more predictors (e.g., temperature, radiation, vapor pressure deficit (VPD), soil water content, precipitation).  \n",
    "   - Explore polynomial regression with degrees ‚â• 2.  \n",
    "\n",
    "4. Prevent overfitting:  \n",
    "   - When using higher-order polynomial models, also test **Ridge** or **Lasso** regularization.  \n",
    "   - Compare performance with and without regularization.  \n",
    "\n",
    "5. Model selection:  \n",
    "   - Choose your **final model**.  \n",
    "   - Provide a rationale: Why this model? Consider the **bias‚Äìvariance tradeoff**, **accuracy**, and **interpretability**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Deliverables**\n",
    "- Report **RMSE** and **R¬≤** for both the training and test sets.  \n",
    "- Provide a plot of **Predicted vs. True Photosynthesis** for the year 2021.  \n",
    "- Submit your **code** along with a concise written explanation (‚â§ 200 words) that:  \n",
    "  - Summarizes your exploration process.  \n",
    "  - Justifies your final model choice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b852a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
